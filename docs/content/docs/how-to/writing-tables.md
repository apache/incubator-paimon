---
title: "Writing Tables"
weight: 4
type: docs
aliases:
- /how-to/writing-tables.html
---
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

# Writing Tables

You can use the `INSERT` statement to inserts new rows into a table 
or overwrites the existing data in the table. The inserted rows can 
be specified by value expressions or result from a query.

## Syntax

```sql
INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }
```
- part_spec

    An optional parameter that specifies a comma-separated list of key and value pairs for partitions. 
    Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.

    Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] )

- column_list

    An optional parameter that specifies a comma-separated list of columns belonging to the 
    table_identifier table.
    
    Syntax: (col_name1 [, column_name2, ...])
    
    {{< hint info >}}

    All specified columns should exist in the table and not be duplicated from each other.
    It includes all columns except the static partition columns.
      
    The size of the column list should be exactly the size of the data from VALUES clause or query.
    
    {{< /hint >}}

- value_expr

    Specifies the values to be inserted. Either an explicitly specified value or a NULL can be 
    inserted. A comma must be used to separate each value in the clause. More than one set of 
    values can be specified to insert multiple rows.

    Syntax: VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]

    {{< hint info >}}

    Currently, Flink doesn't support use NULL directly, so the NULL should be cast to actual 
    data type by `CAST (NULL AS data_type)`.

    {{< /hint >}}

For more information, please check the syntax document:

[Flink INSERT Statement](https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/sql/insert/)

[Spark INSERT Statement](https://spark.apache.org/docs/latest/sql-ref-syntax-dml-insert-table.html)

{{< hint info >}}
Streaming reading will ignore the commits generated by `INSERT OVERWRITE` by default. If you want to read the
commits of `OVERWRITE`, you can configure `streaming-read-overwrite`.
{{< /hint >}}

## Applying Records/Changes to Tables

{{< tabs "insert-into-example" >}}

{{< tab "Flink" >}}

Use `INSERT INTO` to apply records and changes to tables.

```sql
INSERT INTO MyTable SELECT ...
```

Table Store supports shuffle data by bucket in sink phase. To improve data skew, Table Store also
supports shuffling data by partition fields. You can add option `sink.partition-shuffle` to the table.

{{< /tab >}}

{{< tab "Spark3" >}}

Use `INSERT INTO` to apply records and changes to tables.

```sql
INSERT INTO MyTable SELECT ...
```

{{< /tab >}}

{{< /tabs >}}

## Overwriting the Whole Table

For unpartitioned tables, Table Store supports overwriting the whole table.

{{< tabs "insert-overwrite-unpartitioned-example" >}}

{{< tab "Flink" >}}

Use `INSERT OVERWRITE` to overwrite the whole unpartitioned table.

```sql
INSERT OVERWRITE MyTable SELECT ...
```

{{< /tab >}}

{{< /tabs >}}

## Overwriting a Partition

For partitioned tables, Table Store supports overwriting a partition.

{{< tabs "insert-overwrite-partitioned-example" >}}

{{< tab "Flink" >}}

Use `INSERT OVERWRITE` to overwrite a partition.

```sql
INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT ...
```

{{< /tab >}}

{{< /tabs >}}

## Purging tables

You can use `INSERT OVERWRITE` to purge tables by inserting empty value.

{{< tabs "purge-tables-syntax" >}}

{{< tab "Flink" >}}

```sql
INSERT OVERWRITE MyTable SELECT * FROM MyTable WHERE false
```

{{< /tab >}}

{{< /tabs >}}

## Purging Partitions

Currently, Table Store supports two ways to purge partitions.

1. Like purging tables, you can use `INSERT OVERWRITE` to purge data of partitions by inserting empty value to them.

2. Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop-partition job through `flink run`.

{{< tabs "purge-partitions-syntax" >}}

{{< tab "Flink" >}}

```sql
-- Syntax
INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM MyTable WHERE false

-- The following SQL is an example:
-- table definition
CREATE TABLE MyTable (
    k0 INT,
    k1 INT,
    v STRING
) PARTITIONED BY (k0, k1);

-- you can use
INSERT OVERWRITE MyTable PARTITION (k0 = 0) SELECT k1, v FROM MyTable WHERE false

-- or
INSERT OVERWRITE MyTable PARTITION (k0 = 0, k1 = 0) SELECT v FROM MyTable WHERE false
```

{{< /tab >}}

{{< tab "Flink Job" >}}

Run the following command to submit a drop-partition job for the table.

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    drop-partition \
    --warehouse <warehouse-path> \
    --database <database-name> \
    --table <table-name>
    --partition <partition_spec>
    [--partition <partition_spec> ...]

partition_spec:
key1=value1,key2=value2...
```

For more information of drop-partition, see

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    drop-partition --help
```

{{< /tab >}}

{{< /tabs >}}

## Deleting from table

Currently, Table Store supports deleting records via submitting the 'delete' job through `flink run`.

{{< tabs "delete-from-table" >}}

{{< tab "Flink Job" >}}

Run the following command to submit a 'delete' job for the table.

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    delete \
    --warehouse <warehouse-path> \
    --database <database-name> \
    --table <table-name>
    --where <filter_spec>
    
filter_spec is equal to the 'WHERE' clause in SQL DELETE statement. Examples:
    age >= 18 AND age <= 60
    animal <> 'cat'
    id > (SELECT count(*) FROM employee)
```

For more information of 'delete', see

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    delete --help
```

{{< /tab >}}

{{< /tabs >}}

## Merging into table

Table Store supports "MERGE INTO" via submitting the 'merge-into' job through `flink run`. The design referenced such syntax:
```sql
MERGE INTO target-table
  USING source-table | source-expr AS source-alias
  ON merge-condition
  WHEN MATCHED [AND matched-condition]
    THEN UPDATE SET xxx
  WHEN MATCHED [AND matched-condition]
    THEN DELETE
  WHEN NOT MATCHED [AND not-matched-condition]
    THEN INSERT VALUES (xxx)
  WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition]
    THEN UPDATE SET xxx
  WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition]
    THEN DELETE
```

{{< tabs "merge-into" >}}

{{< tab "Flink Job" >}}

Run the following command to submit a 'merge-into' job for the table.

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    merge-into \
    --warehouse <warehouse-path> \
    --database <database-name> \
    --table <target-table>
    --using-table <source-table>
    --on <merge-condition>
    --merge-actions <matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete>
    --matched-upsert-condition <matched-condition>
    --matched-upsert-set <upsert-changes>
    --matched-delete-condition <matched-condition>
    --not-matched-insert-condition <not-matched-condition>
    --not-matched-insert-values <insert-values>
    --not-matched-by-source-upsert-condition <not-matched-by-source-condition>
    --not-matched-by-source-upsert-set <not-matched-upsert-changes>
    --not-matched-by-source-delete-condition <not-matched-by-source-condition>
    
An example:
-- Target table T (pk (k, dt)) is:
+----+------+-------------+-------+
|  k |    v | last_action |    dt |
+----+------+-------------+-------+
|  1 |  v_1 |    creation | 02-27 |
|  2 |  v_2 |    creation | 02-27 |
|  3 |  v_3 |    creation | 02-27 |
|  4 |  v_4 |    creation | 02-27 |
|  5 |  v_5 |    creation | 02-28 |
|  6 |  v_6 |    creation | 02-28 |
|  7 |  v_7 |    creation | 02-28 |
|  8 |  v_8 |    creation | 02-28 |
|  9 |  v_9 |    creation | 02-28 |
| 10 | v_10 |    creation | 02-28 |
+----+------+-------------+-------+

-- Source table S is:
+----+--------+-------+
|  k |      v |    dt |
+----+--------+-------+
|  1 |    v_1 | 02-27 |
|  4 | <NULL> | 02-27 |
|  7 |  Seven | 02-28 |
|  8 | <NULL> | 02-28 |
|  8 |    v_8 | 02-29 |
| 11 |   v_11 | 02-29 |
| 12 |   v_12 | 02-29 |
+----+--------+-------+

-- Supposed SQL is:
MERGE INTO T
USING S
ON T.k = S.k AND T.dt = S.dt
WHEN MATCHED AND (T.v <> S.v AND S.v IS NOT NULL) THEN UPDATE 
  SET v = S.v, last_action = 'matched_upsert'
WHEN MATCHED AND S.v IS NULL THEN DELETE
WHEN NOT MATCHED AND S.k < 12 THEN INSERT 
  VALUES (S.k, S.v, 'insert', S.dt)
WHEN NOT MATCHED BY SOURCE AND (T.dt < '02-28') THEN UPDATE
  SET v = v || '_nmu', last_action = 'not_matched_upsert'
WHEN NOT MATCHED BY SOURCE AND (T.dt >= '02-28') THEN DELETE

-- Matched part: T [(1, 02-27), (4, 02-27), (7, 02-28), (8, 02-28)]
-- Not-matched part: S [(8, 02-29), (11, 02-29), (12, 02-29)]
-- Not-matched-by-source part: T [(2, 02-27), (3, 02-27), (5, 02-28), (6, 02-28) (9, 02-28), (10, 02-28)]

-- equal flink run command is: 
./flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    merge-into \
    --warehouse <warehouse-path> \
    --database <database-name> \
    --table T \
    --using-table S \
    --on "T.k = S.k AND T.dt = S.dt" \
    --merge-actions \
    matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete \
    --matched-upsert-condition "T.v <> S.v AND S.v IS NOT NULL" \
    --matched-upsert-set "v = S.v, last_action = 'matched_upsert'" \
    --matched-delete-condition "S.v IS NULL" \
    --not-matched-insert-condition "S.k < 12" \
    --not-matched-insert-values "S.k, S.v, 'insert', S.dt" \
    --not-matched-by-source-upsert-condition "T.dt < '02-28'" \
    --not-matched-by-source-upsert-set "v = v || '_nmu', last_action = 'not_matched_upsert'" \
    --not-matched-by-source-delete-condition "T.dt >= '02-28'"

-- streaming changes of T after action run:
+----+----+---------+--------------------+-------+
| op |  k |       v |        last_action |    dt |
+----+----+---------+--------------------+-------+
| -U |  7 |     v_7 |           creation | 02-28 | 
| +U |  7 |   Seven |     matched_upsert | 02-28 |
| -D |  4 |     v_4 |           creation | 02-27 |
| -D |  8 |     v_8 |           creation | 02-28 |
| +I |  8 |     v_8 |             insert | 02-29 |
| +I | 11 |    v_11 |             insert | 02-29 |
| -U |  2 |     v_2 |           creation | 02-27 |
| +U |  2 | v_2_nmu | not_matched_upsert | 02-27 |
| -U |  3 |     v_3 |           creation | 02-27 |
| +U |  3 | v_3_nmu | not_matched_upsert | 02-27 |
| -D |  5 |     v_5 |           creation | 02-28 |
| -D |  6 |     v_6 |           creation | 02-28 |
| -D |  9 |     v_9 |           creation | 02-28 |
| -D | 10 |    v_10 |           creation | 02-28 |
+----+----+---------+--------------------+-------+
```

For more information of 'merge-into', see

```bash
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-flink-**-{{< version >}}.jar \
    merge-into --help
```
